# Airbnb Kaggle Competitions'''In this challenge, you are given a list of users along with their demographics, web session records, and some summary statistics. You are asked to predict which country a new user's first booking destination will be. All the users in this dataset are from the USA.There are 12 possible outcomes of the destination country: 'US', 'FR', 'CA', 'GB', 'ES', 'IT', 'PT', 'NL','DE', 'AU', 'NDF' (no destination found), and 'other'. Please note that 'NDF' is different from 'other' because 'other' means there was a booking, but is to a country not included in the list, while 'NDF' means there wasn't a booking.The training and test sets are split by dates. In the test set, you will predict all the new users with first activities after 4/1/2014. In the sessions dataset, the data only dates back to 1/1/2014, while the users dataset dates back to 2010. '''import numpy as npimport pandas as pdfrom sklearn.decomposition import PCAimport matplotlib.pyplot as pltfrom mpl_toolkits.mplot3d import Axes3Dfrom sklearn import preprocessingfrom sklearn import grid_searchfrom sklearn.grid_search import GridSearchCVfrom sklearn.ensemble import GradientBoostingClassifierfrom sklearn.ensemble import RandomForestClassifierage_gender_bkts = pd.read_table('/Users/ianmacomber/Python Work/Kaggle/Airbnb/age_gender_bkts.csv', sep=',')countries = pd.read_table('/Users/ianmacomber/Python Work/Kaggle/Airbnb/countries.csv', sep=',')sample_submission = pd.read_table('/Users/ianmacomber/Python Work/Kaggle/Airbnb/sample_submission.csv', sep=',')sessions = pd.read_table('/Users/ianmacomber/Python Work/Kaggle/Airbnb/sessions.csv', sep=',')test_users = pd.read_table('/Users/ianmacomber/Python Work/Kaggle/Airbnb/test_users.csv', sep=',')train_users = pd.read_table('/Users/ianmacomber/Python Work/Kaggle/Airbnb/train_users.csv', sep=',')'''age_gender_bkts.shape # (420, 5)countries.shape # (10, 7)sample_submission.shape # (87346, 2)sessions.shape # (5600850, 6)test_users.shape # (43673, 15)train_users.shape # (171239, 16)'''# Next steps# Turn Unixtime into something more helpful.  Get first hour and day viewed.# train = train.fillna(-100) ?# Find which columns are having data issues and clean them# Let's investigate some pandas dateparsing and put these into a format that's useable# Turn these into timestampstrain_users['date_account_created'] = pd.to_datetime(train_users['date_account_created'])test_users['date_account_created'] = pd.to_datetime(test_users['date_account_created'])train_users['date_first_booking'] = pd.to_datetime(train_users['date_first_booking'])test_users['date_first_booking'] = pd.to_datetime(test_users['date_first_booking'])# The timestamp_first_active is in a weird int format# 20100104032827# a = 2010-01-04 03:28:27# I'd like to turn this into a string, then turn it back into a timestamp.train_users['timestamp_first_active'] = train_users['timestamp_first_active'].apply(str)test_users['timestamp_first_active'] = test_users['timestamp_first_active'].apply(str)train_users['timestamp_first_active'] = pd.to_datetime(train_users['timestamp_first_active'])test_users['timestamp_first_active'] = pd.to_datetime(test_users['timestamp_first_active'])# I bet that hour of day and day of week is an important part of this# One question is whether this is standardized to a specific time.# train_users['timestamp_first_active'][0].hour      # 4# train_users['timestamp_first_active'][0].weekday() # 3 (Thursday)# train_users['timestamp_first_active'][0].month# Using the timestamp functions here to break a date into its important parts, then dropping the column.train_users['year_account_created'] = train_users['date_account_created'].apply(lambda x: x.year)train_users['month_account_created'] = train_users['date_account_created'].apply(lambda x: x.month)train_users['day_account_created'] = train_users['date_account_created'].apply(lambda x: x.day)# train_users['hour_account_created'] = train_users['date_account_created'].apply(lambda x: x.hour)train_users['weekday_account_created'] = train_users['date_account_created'].apply(lambda x: x.weekday())train_users.drop(['date_account_created'], axis=1,inplace=True)# This actually isn't necessary, because the test set doesn't have this data.# Revisit this because it kind of does, but I don't think we should include for now.# train_users['year_first_booking'] = train_users['date_first_booking'].apply(lambda x: x.year)# train_users['month_first_booking'] = train_users['date_first_booking'].apply(lambda x: x.month)# train_users['day_first_booking'] = train_users['date_first_booking'].apply(lambda x: x.day)# train_users['hour_first_booking'] = train_users['date_first_booking'].apply(lambda x: x.hour)# train_users['weekday_first_booking'] = train_users['date_first_booking'].apply(lambda x: x.weekday())train_users.drop(['date_first_booking'], axis=1,inplace=True)train_users['year_first_active'] = train_users['timestamp_first_active'].apply(lambda x: x.year)train_users['month_first_active'] = train_users['timestamp_first_active'].apply(lambda x: x.month)train_users['day_first_active'] = train_users['timestamp_first_active'].apply(lambda x: x.day)train_users['hour_first_active'] = train_users['timestamp_first_active'].apply(lambda x: x.hour)train_users['weekday_first_active'] = train_users['timestamp_first_active'].apply(lambda x: x.weekday())train_users.drop(['timestamp_first_active'], axis=1,inplace=True)# Same thing for testtest_users['year_account_created'] = test_users['date_account_created'].apply(lambda x: x.year)test_users['month_account_created'] = test_users['date_account_created'].apply(lambda x: x.month)test_users['day_account_created'] = test_users['date_account_created'].apply(lambda x: x.day)# test_users['hour_account_created'] = test_users['date_account_created'].apply(lambda x: x.hour)test_users['weekday_account_created'] = test_users['date_account_created'].apply(lambda x: x.weekday())test_users.drop(['date_account_created'], axis=1,inplace=True)# test_users['year_first_booking'] = test_users['date_first_booking'].apply(lambda x: x.year)# test_users['month_first_booking'] = test_users['date_first_booking'].apply(lambda x: x.month)# test_users['day_first_booking'] = test_users['date_first_booking'].apply(lambda x: x.day)# test_users['hour_first_booking'] = test_users['date_first_booking'].apply(lambda x: x.hour)# test_users['weekday_first_booking'] = test_users['date_first_booking'].apply(lambda x: x.weekday())test_users.drop(['date_first_booking'], axis=1,inplace=True)test_users['year_first_active'] = test_users['timestamp_first_active'].apply(lambda x: x.year)test_users['month_first_active'] = test_users['timestamp_first_active'].apply(lambda x: x.month)test_users['day_first_active'] = test_users['timestamp_first_active'].apply(lambda x: x.day)test_users['hour_first_active'] = test_users['timestamp_first_active'].apply(lambda x: x.hour)test_users['weekday_first_active'] = test_users['timestamp_first_active'].apply(lambda x: x.weekday())test_users.drop(['timestamp_first_active'], axis=1,inplace=True)# Age is fucked up ('age', 1932.0, array([False], dtype=bool))# If the number is bigger than 150 and less than 1995, we want to make the age 2014-age# If the number is greater than 2000, we want to make it n/adef age_cleaner(x):    if x <= 150:        return x    elif x > 150 and x < 1995:        return 2014-x    else:        return np.nan# This looks good# train_users['age'][train_users['age'] >= 120].sort_values().head(50).apply(lambda x: age_cleaner(x))train_users['age'] = train_users['age'].apply(lambda x: age_cleaner(x))test_users['age'] = test_users['age'].apply(lambda x: age_cleaner(x))# Some good summary statscollist = list(train_users.columns.values)collist.remove('id')collist.remove('country_destination')# This gives me all of the top values in a groupfor i in collist:    print(train_users.groupby([i])[i].count().sort_values(ascending=False).head(10))# This gives me all of the items in my training set not in the test setfor i in collist:    for n in train_users[i].unique():        if not np.in1d(n, test_users[i].unique()):           print(i, n, np.in1d(n, test_users[i].unique()))# This gives me all of the items in my test set not in the training setfor i in collist:    for n in test_users[i].unique():        if not np.in1d(n, train_users[i].unique()):           print(i, n, np.in1d(n, train_users[i].unique()))# could not convert string to float: Chrome# So it looks like here we're gonna have to do some label encoding.X = train_users[collist]y = train_users['country_destination']X_test = test_users[collist]for f in X.columns:    if X[f].dtype=='object':        lbl = preprocessing.LabelEncoder()        if f not in X_test.columns:            lbl.fit(np.unique(list(X[f].values)))            X[f] = lbl.transform(list(X[f].values))        else:            lbl.fit(np.unique(list(X[f].values) + list(X_test[f].values)))            X[f] = lbl.transform(list(X[f].values))            X_test[f] = lbl.transform(list(X_test[f].values))# Get rid of the NaNs.  This can be doublechecked.from sklearn.preprocessing import Imputerimp = Imputer(missing_values='NaN', strategy='median', axis=0)x_train_nonan = imp.fit_transform(X)x_test_nonan = imp.fit_transform(X_test)# Run a Random Forest Classifier with 100 estimatorsrfc = RandomForestClassifier(n_estimators=100)rfc.fit(x_train_nonan, y)rfc.feature_importances_zip(X.columns, rfc.feature_importances_)n_estimatorsmax_features=sqrt(n_features) for classification taskszip(X.columns, rfc.feature_importances_)# Use a CV to get better idearfc = GridSearchCV(RandomForestClassifier(), cv=3, verbose=1, n_jobs=-1,                  param_grid={"n_estimators": [50, 100, 200],                              "max_depth": [3, 5],                              "max_features": [10, 20]                              })                              rfc.best_params_ # {'max_depth': 3, 'max_features': 20, 'n_estimators': 50}# Submit ity_test = rfc.predict(x_test_nonan)submission = pd.DataFrame()submission["id"] = test_users["id"]submission["country"] = y_testsubmission.to_csv('airbnb.csv', index=False)# But the goal isn't to submit a single entry, but 5.. right?rfc.predict_proba(x_test_nonan[0])rfc.predict(x_test_nonan[0]) # 'US'rfc.predict(x_test_nonan[3]) # 'NDFrfc.predict_proba(x_test_nonan[0]) # [0.00428182, 0.00986731, 0.0088866, 0.0167438, 0.03509859, 0.0148733, 0.01826256, 0.24607541, 0.0052428, 0.00160856, 0.55562509, 0.08343416]]rfc.predict_proba(x_test_nonan[3]) # [0.00441896, 0.01025697, 0.00812792, 0.01314167, 0.03662052, 0.01685413, 0.01891772, 0.42680476, 0.00547207, 0.0012983, 0.39330142, 0.06478555]])classes = rfc.best_estimator_.classes_zip(rfc.best_estimator_.classes_, rfc.predict_proba(x_test_nonan[0])[0])'''[('AU', 0.0042818241225099666), ('CA', 0.0098673115948438288), ('DE', 0.0088865968694798597), ('ES', 0.016743801519588947), ('FR', 0.035098589148491538), ('GB', 0.014873299721852322), ('IT', 0.018262558498948419), ('NDF', 0.24607541284715939), ('NL', 0.0052427964411712966), ('PT', 0.0016085621445512694), ('US', 0.55562508621202322), ('other', 0.083434160879379868)]'''dict(zip(rfc.best_estimator_.classes_, rfc.predict_proba(x_test_nonan[0])[0])) predictions = rfc.predict_proba(x_test_nonan)rfc.best_estimator_.classes_.shapepredictions.shape[0]submission = pd.DataFrame()submission["id"] = test_users["id"]# np.stack((rfc.best_estimator_.classes_, predictions[0].T), axis=-1)# Getting there# np.stack((np.repeat(submission.iloc[0][0], 12), rfc.best_estimator_.classes_, predictions[0].T), axis=-1)a = pd.DataFrame(np.stack((np.repeat(submission.iloc[0][0], 12), rfc.best_estimator_.classes_, predictions[0].T), axis=-1))# b = pd.DataFrame(np.stack((np.repeat(submission.iloc[1][0], 12), rfc.best_estimator_.classes_, predictions[1].T), axis=-1))# np.stack((np.repeat(submission.iloc[0][0], 12), rfc.best_estimator_.classes_, predictions[0].T), axis=-1)# pd.concat([a, b])# This took a while but was somewhat manageable!for i in xrange(1, predictions.shape[0]+1):    b = pd.DataFrame(np.stack((np.repeat(submission.iloc[i][0], 12), rfc.best_estimator_.classes_, predictions[i].T), axis=-1))    a = pd.concat([a, b])# Now we need to order Aa.columns = ['id', 'country', 'prob']a = a.sort_values(['id', 'prob'], ascending=[True, False])a[['id', 'country']]a[['id', 'country']].to_csv('airbnb.csv', index=False)'''result = []sfor index, row in test.iterrows():    if isinstance(row['date_first_booking'], float):        result.append([row['id'], 'NDF'])        result.append([row['id'], 'US'])        result.append([row['id'], 'other'])        result.append([row['id'], 'FR'])        result.append([row['id'], 'IT'])    else:        result.append([row['id'], 'US'])        result.append([row['id'], 'other'])        result.append([row['id'], 'FR'])        result.append([row['id'], 'IT'])        result.append([row['id'], 'GB'])        pd.DataFrame(result).to_csv('sub.csv', index = False, header = ['id', 'country'])''''''Maximum depth of the individual regression estimators. The maximum depth limits the number of nodes in the tree. Tune this parameter for best performance; the best value depends on the interaction of the input variables. Ignored if max_leaf_nodes is not None.''''''number of regression trees (n_estimators)depth of each individual tree (max_depth)loss function (loss)learning rate (learning_rate)''''''The most important regularization technique for GBRT is shrinkage: the idea is basically to do slow learning by shrinking the predictions of each individual tree by some small scalar, the learning_rate. By doing so the model has to re-enforce concepts. A lower learning_rate requires a higher number of n_estimators to get to the same level of training error — so its trading runtime against accuracy.'''# This is taking way too fucking long to run.# A single one of these takes about 13 min - I assume that this is 8 x 4?  CV might take longer# Probably break into a single test/train for now instead of cv=4# I'm gonna kill this for now.'''clf = GridSearchCV(GradientBoostingClassifier(), cv=4,                  param_grid={"learning_rate": [0.05, 0.1],                              "n_estimators": [200, 300],                              "max_depth": [3, 5]                              }, n_jobs=-1)clf.fit(x_train_nonan, y)clf.best_params_clf.feature_importances_zip(X.columns, clf.feature_importances_)y_test = clf.predict(x_test_nonan)submission = pd.DataFrame()submission["id"] = test_users["id"]submission["country"] = y_testsubmission.to_csv('airbnb.csv', index=False)'''# Look here to get idea code for overfitting# http://www.slideshare.net/DataRobot/gradient-boosted-regression-trees-in-scikitlearn# Should I make this into a fortran layout with float32?# This is how you have a layout of feature importance'''from sklearn import preprocessingfrom sklearn.decomposition import PCAfig = plt.figure(1, figsize=(8, 6))ax = Axes3D(fig, elev=-150, azim=110)X_reduced = PCA(n_components=3).fit_transform(train_users)ax.scatter(X_reduced[:, 0], X_reduced[:, 1], X_reduced[:, 2], c=Y,           cmap=plt.cm.Paired)ax.set_title("First three PCA directions")ax.set_xlabel("1st eigenvector")ax.w_xaxis.set_ticklabels([])ax.set_ylabel("2nd eigenvector")ax.w_yaxis.set_ticklabels([])ax.set_zlabel("3rd eigenvector")ax.w_zaxis.set_ticklabels([])plt.show()ValueError: could not convert string to float: NDF'''# This will probably be an important piece of code# train = pd.merge(train, sessions, how="left", left_on=["id"], right_on=["user_id"])# test = pd.merge(test, sessions, how="left", left_on=["id"], right_on=["user_id"])